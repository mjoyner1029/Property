# .github/workflows/load-test.yml
name: Performance Load Testing

on:
  schedule:
    - cron: "0 2 * * *"   # Nightly at 2 AM UTC
  workflow_dispatch: {}

permissions:
  contents: read

concurrency:
  group: load-test-${{ github.ref }}
  cancel-in-progress: true

jobs:
  load-test:
    name: API Load Test
    runs-on: ubuntu-latest
    timeout-minutes: 30

    env:
      # REQUIRED
      API_BASE_URL: ${{ secrets.STAGING_API_URL }}
      AUTH_TOKEN: ${{ secrets.LOAD_TEST_AUTH_TOKEN }}
      # THRESHOLDS (tune without editing the script)
      P95_TARGET_MS: "300"
      ERROR_RATE_TARGET_PCT: "1.0"
      # K6 test parameters
      VUS: "20"
      DURATION: "60s"

    steps:
      - name: Checkout
        uses: actions/checkout@8ade135a41bc03ea155e62e844d188df1ea18608 # v4.1.0

      - name: Preflight (ensure required secrets)
        id: preflight
        run: |
          ok=true
          [ -z "${API_BASE_URL}" ] && echo "::warning::STAGING_API_URL not set" && ok=false
          [ -z "${AUTH_TOKEN}" ] && echo "::warning::LOAD_TEST_AUTH_TOKEN not set" && ok=false
          echo "can_run=$ok" >> "$GITHUB_OUTPUT"

      - name: Setup k6
        if: steps.preflight.outputs.can_run == 'true'
        uses: grafana/setup-k6@1b6affdd0b94a32e7cd115c14479ae22b0ec67f0 # v1.1.0
        with:
          version: v0.45.0

      - name: Setup Python (for report parsing)
        if: steps.preflight.outputs.can_run == 'true'
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d # v5.0.0
        with:
          python-version: "3.11"

      - name: Run k6 (moderate load)
        if: steps.preflight.outputs.can_run == 'true'
        run: |
          mkdir -p load-test-results
          echo "Running ${VUS} VUs for ${DURATION} vs ${API_BASE_URL}"
          # Export a deterministic summary JSON; keep raw event stream if you want deeper analysis later
          k6 run \
            --vus "${VUS}" \
            --duration "${DURATION}" \
            --tag testType=moderate \
            --summary-export load-test-results/summary.json \
            --out json=load-test-results/raw_results.json \
            perf/load/api_load_test.js

      - name: Build summary markdown + pass/fail
        id: summarize
        if: steps.preflight.outputs.can_run == 'true'
        run: |
          set -e
          : "${P95_TARGET_MS:=300}"
          : "${ERROR_RATE_TARGET_PCT:=1.0}"

          cat > load-test-results/summary.md <<EOM
          # Load Test Results

          **Date:** $(date -u)  
          **Target API:** ${API_BASE_URL}  
          **Test:** ${VUS} VUs for ${DURATION}  
          **Thresholds:** P95 < ${P95_TARGET_MS} ms, Error rate < ${ERROR_RATE_TARGET_PCT}%

          ## Summary
          EOM

          python - <<'PY'
          import json, os, sys
          summary_path = "load-test-results/summary.json"
          with open(summary_path, "r") as f:
            data = json.load(f)

          # k6 summary-export format: metrics -> { metricName: { values: { 'p(95)': ..., 'rate': ... }, ... } }
          metrics = data.get("metrics", {})

          def p95(name):
            return float(metrics.get(name, {}).get("values", {}).get("p(95)", 0.0))

          def rate(name):
            return float(metrics.get(name, {}).get("values", {}).get("rate", 0.0))

          http_p95 = p95("http_req_duration")

          # Custom Trend metrics must be defined in your k6 script (optional)
          props_p95   = p95("properties_latency")
          tenants_p95 = p95("tenants_latency")
          pays_p95    = p95("payments_history_latency")

          err_rate_pct = rate("failed_requests") * 100.0

          p95_target = float(os.environ.get("P95_TARGET_MS", "300"))
          err_target = float(os.environ.get("ERROR_RATE_TARGET_PCT", "1.0"))

          def badge(ok): return "✅ PASS" if ok else "❌ FAIL"

          rows = [
            ("Overall P95 Latency", http_p95, badge(http_p95 < p95_target)),
            ("Properties P95 Latency", props_p95, badge(props_p95 < p95_target) if props_p95 > 0 else "—"),
            ("Tenants P95 Latency", tenants_p95, badge(tenants_p95 < p95_target) if tenants_p95 > 0 else "—"),
            ("Payments P95 Latency", pays_p95, badge(pays_p95 < p95_target) if pays_p95 > 0 else "—"),
            ("Error Rate (%)", err_rate_pct, badge(err_rate_pct < err_target)),
          ]

          with open("load-test-results/summary.md", "a") as out:
            out.write("\n| Metric | Value | Pass/Fail |\n|---|---:|:---:|\n")
            for label, val, ok in rows:
              if ok == "—":
                out.write(f"| {label} | {val:.2f} | — |\n")
              else:
                out.write(f"| {label} | {val:.2f} | {ok} |\n")

          passed = (http_p95 < p95_target) and (err_rate_pct < err_target)
          # Consider custom metrics only if they exist (>0)
          for v in (props_p95, tenants_p95, pays_p95):
            if v > 0:
              passed = passed and (v < p95_target)

          # Emit an output consumable by the workflow
          print(f"passed={'true' if passed else 'false'}")
          PY
          # capture python output
          PASSED=$(python - <<'PY'
          import json, os, sys
          # Re-run minimal pass flag read from previously printed line (above).
          # Simpler: recompute same quick check from the JSON to avoid brittle piping.
          data = json.load(open("load-test-results/summary.json"))
          metrics = data.get("metrics", {})
          def p95(name): return float(metrics.get(name, {}).get("values", {}).get("p(95)", 0.0))
          def rate(name): return float(metrics.get(name, {}).get("values", {}).get("rate", 0.0))
          http_p95 = p95("http_req_duration")
          err_rate_pct = rate("failed_requests") * 100.0
          p95_target = float(os.environ.get("P95_TARGET_MS", "300"))
          err_target = float(os.environ.get("ERROR_RATE_TARGET_PCT", "1.0"))
          # soft check: only core thresholds
          ok = (http_p95 < p95_target) and (err_rate_pct < err_target)
          print("true" if ok else "false")
          PY)
          echo "test_passed=$PASSED" >> "$GITHUB_OUTPUT"

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@26f29941396feab9e9d8596440e50339d00fd9ae # v4.3.0
        with:
          name: load-test-results
          path: load-test-results/
          retention-days: 30

      - name: Post results to GitHub summary
        if: always()
        run: cat load-test-results/summary.md >> "$GITHUB_STEP_SUMMARY"

      - name: Skip (missing secrets)
        if: steps.preflight.outputs.can_run != 'true'
        run: echo "Required secrets missing; skipping load test."

      - name: Enforce thresholds
        if: steps.preflight.outputs.can_run == 'true' && steps.summarize.outputs.test_passed == 'false'
        run: |
          echo "::error::Load test failed! Thresholds not met (P95 < ${P95_TARGET_MS} ms, Error rate < ${ERROR_RATE_TARGET_PCT}%)."
          exit 1
